


import pandas as pd
import numpy as np
from math import log2
import copy
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay





path = "financial_risk_assessment.csv"
df = pd.read_csv(path)
display(df.head())





df.isnull().sum()





for (columnName,columnData) in df.items():
    if columnData.dtype in ['float64', 'int64']:
        df[columnName] = df[columnName].fillna(columnData.mean())
    else:
        df[columnName] = df[columnName].fillna(columnData.mode()[0])





df = df.drop(columns=['City', 'State', 'Country', 'Marital Status Change'])





def bin_numerical_columns(df, num_bins=7):
    """
    Bins numerical columns into 'num_bins' discrete categories.
    Each numerical value is rounded to the nearest bin based on its range.

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - num_bins (int): The number of bins to create for numerical columns.

    Returns:
    - df (pd.DataFrame): The DataFrame with binned numerical columns.
    """
    for column in df.columns:
        # Check if the column is numerical
        if np.issubdtype(df[column].dtype, np.number):
            # Create bins using np.linspace to get evenly spaced bins based on min and max values
            min_val = df[column].min()
            max_val = df[column].max()
            bins = np.linspace(min_val, max_val, num_bins+1)

            # Assign each value to the closest bin
            df[column] = pd.cut(df[column], bins=bins, labels=np.arange(1, num_bins+1), include_lowest=True)

    return df
df = bin_numerical_columns(df)





encoding_maps = {}
def dynamic_encode(column):
    encoding_map = {}
    encoded_values = []
    current_max = 0

    for value in column:
        if value not in encoding_map:
            encoding_map[value] = current_max
            current_max += 1
        encoded_values.append(encoding_map[value])

    return encoded_values, encoding_map

for column in df.columns:
    if df[column].dtype == 'object':
        encoded_column, encoding_map = dynamic_encode(df[column])
        df[column] = encoded_column
        encoding_maps[column] = encoding_map
print("="*50)
print("COLUMN ENCODING MAPPINGS")
print("="*50)

for column_name, mapping in encoding_maps.items():
    print(f"\nColumn: {column_name}")
    print("-"*40)
    for value, code in mapping.items():
        print(f"{code:>3} â†’ {value}")
    print(f"Total categories: {len(mapping)}")





X = df.iloc[:, :-1]
y = df.iloc[:, -1]

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)








def entropy(target_col):
    """
    Calculate entropy of a target column
    """
    elements, counts = np.unique(target_col, return_counts=True)
    entropy = -np.sum([(counts[i]/np.sum(counts)) * log2(counts[i]/np.sum(counts))
                      for i in range(len(elements))])
    return entropy

def info_gain(data, feature, target_name):
    """
    Calculate information gain for splitting on a feature
    """
    total_entropy = entropy(data[target_name])

    # Calculate weighted entropy
    vals, counts = np.unique(data[feature], return_counts=True)
    weighted_entropy = np.sum([(counts[i]/np.sum(counts)) *
                             entropy(data.where(data[feature]==vals[i]).dropna()[target_name])
                             for i in range(len(vals))])

    return total_entropy - weighted_entropy








def id3(data, original_data, features, target_name, max_depth=None, current_depth=0):
    """
    ID3 Algorithm implementation
    """
    # Stopping conditions
    if len(np.unique(data[target_name])) <= 1:
        return np.unique(data[target_name])[0]

    if len(features) == 0 or (max_depth is not None and current_depth == max_depth):
        return np.unique(data[target_name])[np.argmax(np.unique(data[target_name], return_counts=True)[1])]

    # Select best feature
    gains = [info_gain(data, feature, target_name) for feature in features]
    best_feature = features[np.argmax(gains)]

    # Create tree structure
    tree = {best_feature: {}}

    # Remove best feature from features
    features = [f for f in features if f != best_feature]

    # Grow tree recursively
    for value in np.unique(data[best_feature]):
        sub_data = data.where(data[best_feature] == value).dropna()

        if len(sub_data) == 0:
            tree[best_feature][value] = np.unique(original_data[target_name])[
                np.argmax(np.unique(original_data[target_name], return_counts=True)[1])]
        else:
            tree[best_feature][value] = id3(sub_data, original_data, features,
                                           target_name, max_depth, current_depth+1)

    return tree





def predict(tree, sample):
    """
    Predicts the class label for a single sample using the trained decision tree.

    Args:
        tree: The trained decision tree (nested dictionary structure)
        sample: A single data instance (dictionary or pandas Series)

    Returns:
        Predicted class label or None for unseen feature values
    """
    # Get the first/only feature in the current node
    current_feature = next(iter(tree))

    # Get the feature value from the sample
    feature_value = sample[current_feature]

    # Check if this value exists in our tree
    if feature_value in tree[current_feature]:
        subtree = tree[current_feature][feature_value]

        # If we've reached a leaf node, return its value
        if not isinstance(subtree, dict):
            return subtree

        # Otherwise continue recursively
        return predict(subtree, sample)

    # Handle unseen feature values
    return None





from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Train the model on the training set
decision_tree = id3(
    data=pd.concat([X_train, y_train], axis=1),
    original_data=pd.concat([X_train, y_train], axis=1),
    features=list(X_train.columns),
    target_name=y_train.name,
    max_depth=4
)

# Function to predict on a DataFrame
def predict_df(tree, X):
    predictions = []
    for i in range(len(X)):
        pred = predict(tree, X.iloc[i])
        predictions.append(pred)
    return predictions

# Get predictions
y_val_pred = predict_df(decision_tree, X_val)
y_test_pred = predict_df(decision_tree, X_test)

# Handle None predictions by replacing them with most common class
most_common_class = y_train.mode()[0]
y_val_pred = [p if p is not None else most_common_class for p in y_val_pred]
y_test_pred = [p if p is not None else most_common_class for p in y_test_pred]

# Plotting confusion matrix
def plot_confusion_matrix(y_true, y_pred, title):
    labels = sorted(list(set(y_true) | set(y_pred)))
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels)
    plt.title(title)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.show()
    return cm, labels

# Manual metric calculation
def manual_metrics(cm, labels):
    total = cm.sum()
    correct = 0
    precisions = []
    recalls = []
    f1s = []

    for i in range(len(cm)):
        TP = cm[i][i]
        FP = sum(cm[:, i]) - TP
        FN = sum(cm[i, :]) - TP

        if (TP + FP) > 0:
            precision = TP / (TP + FP)
        else:
            precision = 0

        if (TP + FN) > 0:
            recall = TP / (TP + FN)
        else:
            recall = 0

        if (precision + recall) > 0:
            f1 = (2 * precision * recall) / (precision + recall)
        else:
            f1 = 0

        precisions.append(precision)
        recalls.append(recall)
        f1s.append(f1)
        correct += TP

    accuracy = correct / total if total > 0 else 0

    print(f"Accuracy: {accuracy:.4f}")
    print("Precision per class:")
    for lbl, val in zip(labels, precisions):
        print(f"  {lbl}: {val:.4f}")
    print("Recall per class:")
    for lbl, val in zip(labels, recalls):
        print(f"  {lbl}: {val:.4f}")
    print("F1 Score per class:")
    for lbl, val in zip(labels, f1s):
        print(f"  {lbl}: {val:.4f}")
    print("Macro Averages:")
    print(f"  Precision: {np.mean(precisions):.4f}")
    print(f"  Recall:    {np.mean(recalls):.4f}")
    print(f"  F1 Score:  {np.mean(f1s):.4f}")

# Test Set Evaluation
print("Test Set Evaluation")
print("-" * 50)
cm_test, labels_test = plot_confusion_matrix(y_test, y_test_pred, "Test Set Confusion Matrix")
manual_metrics(cm_test, labels_test)

# Validation Set Evaluation
print("\nValidation Set Evaluation")
print("-" * 50)
cm_val, labels_val = plot_confusion_matrix(y_val, y_val_pred, "Validation Set Confusion Matrix")
manual_metrics(cm_val, labels_val)












from matplotlib import cm


def get_weakest_twigs_with_ig(tree, X_train, y_train, target_name, top_n=5, print_output=True):
    """Analyze the tree and return the top-n weakest twigs based on Information Gain"""
    full_data = pd.concat([X_train, y_train], axis=1)
    node_info = []

    def traverse(node, path=[]):
        if isinstance(node, dict):
            for feature, branches in node.items():
                subset = full_data.copy()
                for f, v in zip(path[::2], path[1::2]):
                    subset = subset[subset[f] == v]

                if subset.empty:
                    continue

                current_ig = info_gain(subset, feature, target_name)
                class_dist = subset[target_name].value_counts(normalize=True).to_dict()

                node_info.append({
                    'path': path + [feature],
                    'ig': current_ig,
                    'class_dist': class_dist,
                    'n_samples': len(subset)
                })

                for value, subtree in branches.items():
                    traverse(subtree, path + [feature, value])

    traverse(tree)

    node_report = pd.DataFrame(node_info).sort_values('ig').reset_index(drop=True)

    if print_output:
        print(f"\nTop {top_n} weakest paths to prune:")
        for i, row in node_report.head(top_n).iterrows():
            print(f"{i+1}. {' â†’ '.join(map(str, row['path']))} (IG: {row['ig']:.4f}, Samples: {row['n_samples']})")

    return node_report.head(top_n)['path'].tolist()


def prune_paths_by_majority_class(tree, paths_to_prune, X_train, y_train, X_val, y_val, target_name):
    """Prune given paths and evaluate performance after each pruning"""
    current_tree = copy.deepcopy(tree)
    majority_class = y_train.mode()[0]

    # Helper: prune specific path
    def prune_path(tree, path):
        node = tree
        for step in path[:-1]:
          if isinstance(node, dict):
            node = node.get(step, {})
          else:
            # Weâ€™ve reached a leaf too early, abort pruning this path
            return

        last_feature = path[-1]

        subset = pd.concat([X_train, y_train], axis=1)
        for feature, value in zip(path[::2], path[1::2]):
            subset = subset[subset[feature] == value]

        majority = subset[target_name].mode()[0] if not subset.empty else majority_class

        if isinstance(node, dict) and last_feature in node:
            for val in node[last_feature]:
                node[last_feature][val] = majority

    # Initial evaluation
    y_val_pred = predict_df(current_tree, X_val)
    y_val_pred = [pred if pred is not None else majority_class for pred in y_val_pred]
    last_acc = accuracy_score(y_val, y_val_pred)
    print(f"\nInitial validation accuracy: {last_acc:.4f}")

    for i, path in enumerate(paths_to_prune, 1):
        print(f"\nPruning path {i}: {' â†’ '.join(map(str, path))}")
        prune_path(current_tree, path)

        y_val_pred = predict_df(current_tree, X_val)
        y_val_pred = [pred if pred is not None else majority_class for pred in y_val_pred]
        current_acc = accuracy_score(y_val, y_val_pred)
        print(f"New accuracy: {current_acc:.4f} (Î”{current_acc - last_acc:+.4f})")
        last_acc = current_acc

    return current_tree
# Step 1: Get weakest twigs
weakest_paths = get_weakest_twigs_with_ig(decision_tree, X_train, y_train, y_train.name, top_n=5)

# Step 2: Prune and evaluate
pruned_tree = prune_paths_by_majority_class(
    decision_tree,
    weakest_paths,
    X_train, y_train,
    X_val, y_val,
    y_train.name
)

# Final performance
print("\nFinal Pruned Tree Performance:")
y_val_pred = predict_df(pruned_tree, X_val)
# Ensure all predictions are not None
y_val_pred = [pred if pred is not None else y_train.mode()[0] for pred in y_val_pred]
plot_confusion_matrix(y_val, y_val_pred, "Validation Set Confusion Matrix On Pruned Tree")
print(classification_report(y_val, y_val_pred, zero_division=0))



